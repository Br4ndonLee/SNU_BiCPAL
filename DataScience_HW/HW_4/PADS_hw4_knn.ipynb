{"cells":[{"cell_type":"markdown","metadata":{"id":"f7MWUDI2IBYW"},"source":["## Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsEbh_t58ZJo"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GXMM3Js7hII"},"outputs":[],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","#%cd 'COPY&PASTE FILE DIRECTORY HERE'"]},{"cell_type":"markdown","metadata":{"id":"rR3VeoqMIFbq"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"JAC2qQEy7rM8"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{"id":"jMw488CqIOhm"},"source":["## K-Nearest Neighbor Implementation"]},{"cell_type":"markdown","metadata":{"id":"q48EtJ9t2T9m"},"source":["Complete (a) the **train**, (b) the **compute_distance** and (c) the **predict_labels** method.\n","The definitions of various distance metrics used in **compute_distance**, are given as below. Case using the dot product method is already implemented. Complete the remaining cases for using remaining distance metrics.\n","\n","* Dot product distance: $d(x_i, x_j) = 1 - x_i \\cdot x_j$\n","* Cosine distance: $d(x_i, x_j) = 1 - cos(x_i, x_j)$, where $cos()$ returns the cosine of the angle between two input vectors.\n","* L1 distance: $d(x_i, x_j) = \\sum_{k=1}^{n} |x_{ik} - x_{jk}|$\n","* L2 (Euclidean) distance: $d(x_i, x_j) = \\sqrt{\\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}$"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"zpsV2Bb3r3s4"},"outputs":[],"source":["class KNN:\n","    \"\"\" k-nearest neighbor classifier class \"\"\"\n","    def __init__(self):\n","        self.X_train = None\n","        self.y_train = None\n","\n","    def train(self, X, y):\n","        \"\"\"\n","        Train the classifier using the given training data (X, y).\n","        Recall that for k-nearest neighbors this is just memorizing the training data.\n","\n","        Question (a)\n","\n","        Inputs\n","        - X: A numpy array of shape (N, D), where N is the number of data points,\n","            D is the dimensionality of each data point.\n","        - y: A numpy array of shape (N,) containing the training labels, where\n","            y[i] is the label for X[i]. With C classes, each y[i] is an integer\n","            from 0 to C-1.\n","        \"\"\"\n","        ##### YOUR CODE HERE #####\n","        self.X_train = X\n","        self.y_train = y\n","        ##########################\n","\n","    def inference(self, X_test, k=1, dist_metric='dot'):\n","        \"\"\"\n","        For each test example in X, this method predicts its label by majority vote\n","        from the k nearest training samples. It returns the predicted labels.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n","            D is the dimensionality of each data point.\n","        - X_train: A numpy array of shape (M, D), where M is the number of training data points,\n","            D is the dimensionality of each data point.\n","        - k: The number of neighbors to participate in voting.\n","            dist_metric: Determines the distance metric to use. The default is dot-product ('dot'),\n","            but you will need to implement 'L2' for question (b).\n","\n","        Returns\n","        - y_pred: A numpy array of shape (N,) containing predicted labels for the test data X,\n","            where y_pred[i] is the predicted label for the test point X[i].\n","        \"\"\"\n","        dists = self.compute_distance(X_test, dist_metric)\n","        y_pred = self.predict_labels(X_test, dists, k)\n","        return y_pred\n","\n","    def compute_distance(self, X_test, dist_metric='L2'):\n","        \"\"\"\n","        Computes the distance between the training data and test data,\n","        using dot-product similarity or Euclidean (L2) distance as the distance metric.\n","        Use self.X_train to retrieve training data.\n","\n","        Question (b)\n","\n","        Inputs\n","        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n","            D is the dimensionality of each data point.\n","        - dist_metric: Determines the distance metric to use.\n","\n","        Returns\n","        - dists: A numpy array of shape (N, M) where N is the number of test data points,\n","            and M is the number of traininig data points, containing distances between\n","            each pair of test and train data  points based on the given distance metric.\n","        \"\"\"\n","\n","        ##### YOUR CODE HERE #####\n","        if dist_metric=='Dot':\n","            dists = np.dot(X_test, self.X_train.T)\n","\n","        elif dist_metric=='Cos':\n","            norm_X_test = np.linalg.norm(X_test, axis=1, keepdims=True)\n","            norm_X_train = np.linalg.norm(self.X_train, axis=1, keepdims=True)\n","            dot_product = np.dot(X_test, self.X_train.T)\n","            dists = dot_product / (norm_X_test * norm_X_train)\n","\n","        elif dist_metric == 'L1':\n","            dists = np.sum(np.abs(self.X_train - X_test[:, np.newaxis]), axis=2)\n","            \n","        elif dist_metric == 'L2':\n","            dists = np.sqrt(np.sum((self.X_train - X_test[:, np.newaxis])**2, axis=2))\n","\n","        return dists\n","        ##########################\n","    \n","    def predict_labels(self, X_test, dists, k):\n","        \"\"\"\n","        For the given test image, this method takes a majority vote from k closest points\n","        to predict the class of the test image.\n","\n","        Question (c)\n","\n","        Inputs\n","        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n","            D is the dimensionality of each data point.\n","        - dists: A numpy array of shape (N, M) where N is the number of test data points,\n","            and M is the number of traininig data points, containing distances between\n","            each pair of test and train data points based on the given distance metric.\n","        - k: The number of neighbors to participate in voting.\n","\n","        Returns\n","        - y_pred: A numpy array of shape (N,) containing predicted labels for the test data X,\n","            where y_pred[i] is the predicted label for the test point X[i].\n","        \"\"\"\n","\n","        ##### YOUR CODE HERE #####\n","        num_test = X_test.shape[0]\n","        y_pred = np.zeros(num_test)\n","        \n","        for i in range(num_test):\n","            closest_y = []\n","            k_closest = np.argsort(dists[i])[:k]\n","            closest_y = tuple(self.y_train[k_closest])\n","            vote = Counter(closest_y)\n","            y_pred[i] = vote.most_common(1)[0][0]\n","        return y_pred\n","\n","        ##########################\n","\n","    def evaluate(self, y, y_hat):\n","        \"\"\"\n","        Compares the predicted labels to the ground truth y, and prints the\n","        classification accuracy.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - y: A numpy array of shape (N,) containing the ground truth labels, where\n","            N is the number of test examples. With C classes, each y[i] is an integer\n","            from 0 to C-1.\n","        - y_hat: A numpy array of shape (N,) containing the predicted labels, where\n","            N is the number of test examples. With C classes, each y_pred[i] is\n","            an integer from 0 to C-1.\n","\n","        Returns:\n","        - accuracy\n","        \"\"\"\n","        y_hat = np.expand_dims(y_hat, axis=1)\n","        num_correct = np.sum(y_hat == y)\n","        accuracy = float(num_correct) / y.shape[0]\n","        return accuracy"]},{"cell_type":"markdown","metadata":{"id":"yvwolO5qHzXl"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"zR9dOpGX_0iz"},"outputs":[],"source":["def sample_data(X, y, count):\n","    mask = np.random.choice(X.shape[0], count, replace=False)\n","    X_sampled = X[mask]\n","    y_sampled = y[mask]\n","    return X_sampled, y_sampled"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"UOzrdgGSHVXF"},"outputs":[{"name":"stdout","output_type":"stream","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 1)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 1)\n","\n","Training data shape:  (1000, 784)\n","Training labels shape:  (1000, 1)\n","Test data shape:  (200, 784)\n","Test labels shape:  (200, 1)\n"]}],"source":["num_train_data = 1000\n","num_test_data = 200\n","\n","X_train_src, y_train_src, X_test_src, y_test_src = load_data(one_hot_encoding=False) # Training data is flattened when it is loaded\n","X_train, y_train = sample_data(X_train_src, y_train_src, num_train_data)\n","X_test, y_test = sample_data(X_test_src, y_test_src, num_test_data)\n","\n","print()\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"lZDHMwziIpja"},"source":["## Model Training & Evaluation"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"nfq-3_r4IsHB"},"outputs":[],"source":["model = KNN()\n","model.train(X_train, y_train)"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"fPmKakDuuc8b"},"outputs":[{"ename":"TypeError","evalue":"unhashable type: 'numpy.ndarray'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb 셀 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mModel usage for test.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m K \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mtuple\u001b[39m(y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minference(X_test, k\u001b[39m=\u001b[39;49mK, dist_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDot\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(y_test, y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuarcy:\u001b[39m\u001b[39m\"\u001b[39m, acc)\n","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb 셀 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mFor each test example in X, this method predicts its label by majority vote\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfrom the k nearest training samples. It returns the predicted labels.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m    where y_pred[i] is the predicted label for the test point X[i].\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m dists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_distance(X_test, dist_metric)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_labels(X_test, dists, k)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_pred\n","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb 셀 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     k_closest \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(dists[i])[:k]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     closest_y \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train[k_closest])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     vote \u001b[39m=\u001b[39m Counter(closest_y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     y_pred[i] \u001b[39m=\u001b[39m vote\u001b[39m.\u001b[39mmost_common(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_knn.ipynb#X16sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_pred\n","File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39msuper\u001b[39m(Counter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(iterable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n","File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[39msuper\u001b[39m(Counter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mupdate(iterable) \u001b[39m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         _count_elements(\u001b[39mself\u001b[39;49m, iterable)\n\u001b[1;32m    638\u001b[0m \u001b[39mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwds)\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"]}],"source":["\"\"\"\n","Model usage for test.\n","\"\"\"\n","K = 15\n","\n","y_pred = model.inference(X_test, k=K, dist_metric='Dot')\n","acc = model.evaluate(y_test, y_pred)\n","print(\"Accuarcy:\", acc)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_-IRMbqqRtmh"},"source":["## Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0IsD2pwgFxM"},"outputs":[],"source":["# Modify the number of k's and metrics to try as you want\n","num_ks = 30\n","metrics = ['Dot', 'Cos', 'L1', 'L2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68AnUY2wSkMX"},"outputs":[],"source":["# Run experiments\n","print_k_interval = 5\n","result = dict(zip(metrics, [[] for _ in range(len(metrics))]))\n","for metric in metrics:\n","    print(\"running KNN with {} distance metric\".format(metric))\n","    for k in range(1, num_ks+1):\n","        if k % print_k_interval==0:\n","            print(\"    processing... k={:3d}\".format(k))\n","        y_pred = model.inference(X_test, k=k, dist_metric=metric)\n","        acc = model.evaluate(y_test, y_pred)\n","        result[metric].append(acc)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q47KrG6ch6MH"},"outputs":[],"source":["# Visualize the result\n","fig = plt.figure(figsize=(10,6))\n","ax = fig.add_subplot(1,1,1)\n","\n","x_axis = np.arange(1, num_ks+1, 1)\n","for i, metric in enumerate(metrics):\n","    ax.scatter(x_axis, result[metric], label = metric)\n","\n","ax.set(title=\"K-Nearest Neighbor Accuracies on different Ks\")\n","ax.set(xlabel='K', ylabel='Accuracy')\n","ax.set(xticks=np.arange(0, num_ks+1,5), yticks=np.arange(0.5,1.0,0.05))\n","ax.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ivOU_TXAuZrI"},"source":["**Question (e)**:\n","- Interpret the result from the above graph. What distance metric would you use for MNIST classification, and why? Can you discuss why that distance is appropriate for MNIST classification?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eNBd8Zugv189"},"source":["Your answer: 그래프는 k와 distance metrics의 다양한 조합에 대한 분류 정확도를 보여준다. 일반적으로 k 값에 따라 정확도가 어떻게 변하는지 관찰할 수 있는데, k 값이 작을수록 데이터의 local details를 볼 수 있지만 노이즈에 민감할 수 있다. 반대로 k 값이 클수록 더 정확한 예측을 보여주지만 local patterns를 스무스하게 만들 수 있다.\n","거리 측정법은 다양한 방식으로 데이터 포인트 간의 유사성을 측정하는데, 그래프는 MNIST 분류에 대해 각 거리 측정 항목이 얼마나 잘 수행되는지 표시해준다.\n","\n","L2(유클리드) distance는 이미지 분류 작업에 일반적으로 사용된다. 다차원 공간에서 두 점 사이의 직선 거리를 측정한다. 필셀 값의 맥락에서 L2 거리는 로컬과 글로벌 패턴 모두를 고려하여 픽셀 간의 공간적 관계를 표시한다.\n","\n","Cosine Similarity의 경우 두 벡터 사이의 각도 코사인을 측정한다. 이는 크기 변화에 적합하며 벡터의 크기가 크게 중요하지 않은 작업에 자주 사용된다. 이미지의 경우 절대값보다 픽셀값의 방향이 더 중요할 경우 cosine similarity가 더 효과적일 수 있다.\n","\n","L2와 cosine 사이의 선택은 데이터의 특성과 모델이 표현하려는 패턴에 따라 달라진다. L2 distance는 이미지 분류 작업에 적합한 기본 방법일 수 있지만 성능은 데이터의 특성에 따라 달라질 수 있다. 또한 특정 데이터 세트에 가장 적합한 조합을 찾기 위해 다양한 distance metrics와 k 값을 가지고 실험하는 것이 일반적이다. 특히 MNIST가 아닌 더 큰 대규모의 데이터 세트를 처리할 떄 각 지표와 관련된 계산 비용을 고려하는 것이 중요하다."]},{"cell_type":"markdown","metadata":{"id":"3U4DXLMvgD0d"},"source":["-  If you select the best model based on the results and observe a slight performance drop on unseen datasets, what could be the potential reasons contributing to this outcome? How can you resolve this issue?"]},{"cell_type":"markdown","metadata":{"id":"3NbJ2H4DgE0N"},"source":["Your answer : 데이터 세트에서 모델의 성능이 떨어지면 과적합 때문일 수 있다. 과적합은 모델이 너무 복잡하거나 훈련 데이터의 노이즈를 포착하여 일반화 성능이 저하될 때 발생한다. 과적합을 방지하기 위해 cross validation, regularization, 또는 early stopping과 같은 기술을 사용할 수 있다. 성능 저하의 또 다른 이유는 테스트 세트에 이상한 값이나 노이즈가 있는 데이터가 존재할 수 있기 때문이다. 이러한 경우, 모델을 학습시키기 전에 데이터를 전처리하고 이상값이나 노이즈가 있는 샘플을 제거하는 것이 중요하다.\n","\n","이 문제를 해결하기 위해 regularization, dropout 또는 early stopping과 같은 기술을 사용하여 과적합을 방지할 수 있다. Regularization은 손실 함수에 페널티 항을 추가하여 큰 weight를 억제하고, 드롭아웃은 훈련 중에 일부 뉴런을 무작위로 탈락시켜 co-adaption을 방지한다. early stopping은 validation error가 개선되지 않을 때 학습 프로세스를 중지하여 모델이 학습 데이터에 과적합되는 것을 방지한다. 또한 모델을 훈련하기 전에 데이터를 전처리하고 이상값이나 노이즈가 있는 샘플을 제거하는 것도 중요하다."]}],"metadata":{"colab":{"provenance":[{"file_id":"1VqAFzvbyMlb1DqtezXnck7p9_u4_O5aV","timestamp":1692576952298}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
