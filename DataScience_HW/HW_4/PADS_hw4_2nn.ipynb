{"cells":[{"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"er0RD438gRLm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jfeql_8sgnKJ"},"outputs":[],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","#%cd 'COPY&PASTE FILE DIRECTORY HERE'"]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"OyammZP8hI7P"},"outputs":[],"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Akohn4cju7SX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting python-mnist\n","  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n","Installing collected packages: python-mnist\n","Successfully installed python-mnist-0.7\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install python-mnist"]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["#Utils"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"xuQB6W2U5ZE2"},"outputs":[],"source":["def leaky_relu(z, alpha=0.01):\n","    \"\"\"\n","    Implement the leaky ReLU activation function.\n","    The method takes the input z and returns the output of the function.\n","\n","    Set the value of alpha for the leaky ReLU funtion to 0.01.\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE HERE #####\n","    return np.maximum(alpha * z, z)\n","    ##########################\n","\n","def softmax(X):\n","    \"\"\"\n","    Implement the softmax function.\n","    The method takes the input X and returns the output of the function.\n","\n","    Question (a)\n","    \"\"\"\n","\n","    ##### YOUR CODE HERE #####\n","    exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n","    return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n","\n","    ##########################\n","\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch\n","\n","def plot_dataset(images, labels, grid_width, grid_height, figure_width=5, figure_height=5, y_hats=None):\n","    \"\"\"\n","    Plots image and labels.\n","\n","    Do NOT modify this method.\n","    \"\"\"\n","    f, ax = plt.subplots(grid_height, grid_width)\n","    f.set_size_inches(figure_width, figure_height)\n","    img_idx = 0\n","    for i in range(0, grid_height):\n","        for j in range(0, grid_width):\n","            image = images[img_idx]\n","            label = labels[img_idx]\n","            y_hat = Y_hat[img_idx]\n","            label_idx = int(np.argmax(label))\n","            y_hat_idx = int(np.argmax(y_hat))\n","            ax[i][j].axis('off')\n","            ax[i][j].set_title(f'Pred: {y_hat_idx}, Real: {label_idx}', color='k')\n","            ax[i][j].imshow(image, aspect='auto')\n","            img_idx += 1\n","        plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0.25)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["#2-Layer Neural Network"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"mA5udiGmhRb5"},"outputs":[],"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Question (b)\n","\n","        initializes parameters with He Initialization.\n","        - refer to https://paperswithcode.com/method/he-initialization for He intialization\n","\n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","\n","        ##### YOUR CODE HERE #####\n","        params = {}\n","        params['W1'] = np.random.randn(input_dim, num_hiddens) * np.sqrt(2 / input_dim)\n","        params['b1'] = np.zeros((1, num_hiddens))\n","        params['W2'] = np.random.randn(num_hiddens, num_classes) * np.sqrt(2 / num_hiddens)\n","        params['b2'] = np.zeros((1, num_classes))\n","        return params\n","        ##########################\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Defines and performs the feed forward step of a two-layer neural network.\n","        Specifically, the network structue is given by\n","\n","          y = softmax(leaky_relu(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (c)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","\n","        ##### YOUR CODE HERE #####\n","        W1, b1, W2, b2 = self.params\n","        Z1 = np.dot(X, W1) + b1\n","        A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # Leaky ReLU activation\n","        Z2 = np.dot(A1, W2) + b2\n","        \n","        y = self.softmax(Z2)\n","        ff_dict = {'X': X, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'y': y}\n","\n","        return y, ff_dict\n","        ##########################\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (d)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        ##### YOUR CODE HERE #####\n","        grads = {}\n","        B = X.shape[0]  # Batch size\n","        W1, b1, W2, b2 = self.params\n","        Z1, A1, Z2, y = ff_dict\n","\n","        # Compute the gradient of the loss with respect to the output layer\n","        dZ2 = y - Y\n","        grads['W2'] = np.dot(A1.T, dZ2) / B\n","        grads['b2'] = np.sum(dZ2, axis=0, keepdims=True) / B\n","\n","        # Compute the gradient of the loss with respect to the first hidden layer\n","        dA1 = np.dot(dZ2, W2.T)\n","        dZ1 = np.where(Z1 > 0, dA1, 0.01 * dA1)\n","        grads['W1'] = np.dot(X.T, dZ1) / B\n","        grads['b1'] = np.sum(dZ1, axis=0, keepdims=True) / B\n","\n","        return grads\n","        ##########################\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        epsilon = 1e-10\n","        Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n","        Y = Y.astype(float)\n","        Y_hat = Y_hat.astype(float)\n","        \n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Question (e)\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        ##### YOUR CODE HERE #####\n","        y_hat, ff_dict = self.forward(X_batch)\n","\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","\n","        self.params['W1'] -= lr * grads['W1']\n","        self.params['b1'] -= lr * grads['b1']\n","        self.params['W2'] -= lr * grads['W2']\n","        self.params['b2'] -= lr * grads['b2']\n","        ##########################\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","\n","        Question (f)\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"\n","        ##### YOUR CODE HERE #####\n","        correct_predictions = np.sum(np.argmax(Y, axis=1) == np.argmax(Y_hat, axis=1))\n","        total_samples = len(Y)\n","        accuracy = correct_predictions / total_samples\n","        return accuracy\n","        ##########################"]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["#Load MNIST"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"48ooR6YIxYhC"},"outputs":[{"name":"stdout","output_type":"stream","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 10)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 10)\n","\n","Set validation data aside\n","Training data shape:  (48000, 784)\n","Training labels shape:  (48000, 10)\n","Validation data shape:  (12000, 784)\n","Validation labels shape:  (12000, 10)\n"]}],"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["#Training & Evaluation"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"IlnC_rerHPaN"},"outputs":[],"source":["###\n","# Question (f)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"TTCqVT4S0Tm5"},"outputs":[],"source":["# model instantiation\n","model = TwoLayerNN(input_dim=784, num_hiddens=64, num_classes=10)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"6cWb6xg0NxOs"},"outputs":[{"ename":"UFuncTypeError","evalue":"ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U2')) -> None","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb 셀 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr, n_epochs, batch_size \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m256\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb 셀 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X_batch, Y_batch \u001b[39min\u001b[39;00m load_batch(X, Y, batch_size):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(X_batch, Y_batch, batch_size, lr)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m log_interval\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m         Y_hat, ff_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X)\n","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb 셀 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mUpdates the parameters using gradient descent.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39m- lr\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m \u001b[39m##### YOUR CODE HERE #####\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m y_hat, ff_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(X_batch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(X_batch, Y_batch, ff_dict)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m]\n","\u001b[1;32m/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb 셀 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m##### YOUR CODE HERE #####\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m W1, b1, W2, b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m Z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X, W1) \u001b[39m+\u001b[39m b1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m A1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(Z1 \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, Z1, \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m Z1)  \u001b[39m# Leaky ReLU activation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brandon/Documents/SNU_BiCPAL/DataScience_HW/HW_4/PADS_hw4_2nn.ipynb#X21sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m Z2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(A1, W2) \u001b[39m+\u001b[39m b2\n","File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U2')) -> None"]}],"source":["# train the model\n","lr, n_epochs, batch_size = 2.0, 20, 256\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpPsAlXU0T_Z"},"outputs":[],"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_stWORHQJ6J"},"outputs":[],"source":["###\n","# Question (g)\n","# Visualize your inaccurate results and briefly guess why the model may have predicted these numbers incorrectly\n","X_test.shape\n","\n","Y_hat, _ = model.forward(X_test)\n","\n","classes_pred = np.argmax(Y_hat, axis=1)\n","classes_gt = np.argmax(Y_test, axis=1)\n","\n","images = X_test[classes_pred != classes_gt][:16]\n","images = np.reshape(images, (-1, 28, 28))\n","labels = Y_test[classes_pred != classes_gt][:16]\n","Y_hat = Y_hat[classes_pred != classes_gt][:16]\n","plot_dataset(images, labels, grid_width=8, grid_height=2, figure_width=20, figure_height=5, y_hats = Y_hat)"]},{"cell_type":"markdown","metadata":{"id":"6ynY7T4ZXOjs"},"source":["**Question (g)** :  Report the best combination of hyperparameters you find along with your final test accuracy"]},{"cell_type":"markdown","metadata":{"id":"AaVKZmNOhLTA"},"source":["\n","You Answer :"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ZR-2pQFEibTWIAA3wrVJyQRF_XIEQtQQ","timestamp":1669185771597}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
